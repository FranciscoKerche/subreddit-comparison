{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Social Data Science \n",
    "## Week 3 Day 2. Lab. Text Processing \n",
    "\n",
    "In this lab I have generated some methods that will allow you to download posts from Reddit. It accepts a list of subreddits of arbitrary length, which are each processed independently and stored in a single `results` dictionary. The keys of the dictionary are the subreddits. Underneath each subreddit is a dictionary of sub-specific result objects, like \"vectorizer\" and \"top terms\".\n",
    "\n",
    "Please read through the code. You will need to add your username. The code is intentionally broken so you will need to add that before running this. Other than that you should not need to make any modifications to the cell with the `RedditScraper` class. \n",
    "\n",
    "In the cell below is some code to run these methods. At the top are some parameters that you should set. These are typically written in ALL CAPS. You should read the code to understand what they do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises: \n",
    "    \n",
    "0. **Explore subreddits**. The below code uses 'ukpolitics', 'unitedkingdom', and 'uknews'. These were loosely motivated by an interest in whether uknews has become a reactionary subreddit with generally conservative opinions. By comparing it to the other two (which other work has suggested are generally quite similar), we might get a sense of this from the top keywords. Try some other subs related to a topic where you suspect there will be some interesting distinctions. Motivate the distinctions. If you aren't sure about the subs, query an LLM (they are typically trained on a _lot_ of Reddit data and will know good subs). So instead of trying for /r/men and /r/women, if you ask about subs for gender-based interests, it might suggest /r/TwoXChromosomes and /r/MensRights as interesting distinctions. \n",
    "1. **Understand the results data structure**. The `results` object returns the top 5 terms. How would you access more than 5 terms? Expand the results to see 10. Consider what way is more general and flexible. How might you change the code so that there is a `TOP_N = 10` which is then passed through the code so that the results dictionary contains ten terms in the \"top_terms\" DataFrame rather than hard coding it in the method below?\n",
    "2. **Store results**. Every time we run the code we query Reddit again. How can we store our data so that it is cached for another round? There are many approaches to this and among your group you may discover everything from 'just save the json' to 'DataFrame and then export to feather' to some who would ambitiously use MongoDB. Given this is a simple exercise for now, keep this step simple as you need it to be while still usable enough if you want to add more data.\n",
    "3. **Plot keywords over time**. Expand your results to anywhere from 250 upwards (I would here cap at 500 max and think that the api might only return last 1000 but untested). Determine the top keywords using TFIDF. Then plot the frequency of these keywords over this time period for these results.   \n",
    "4. **Table the most common URLs for stories**. Triangulate these plots with a table summarising the top news outlets for this sub in this time period. Notice the starter code to process this from the posts data that has been stored in a large `submissions` dictionary. Note, this code does not turn all the `json` into a DataFrame, but extracts only the URL column and processes that. It also uses a _regular expression_ to separate out the top level domain, which may or may not be the most robust.  \n",
    "5. **Write a summary**. Solely for reflection at this point, write some intuitions that you discover with this exploration. \n",
    "\n",
    "## Caveats for the exercise: \n",
    "- Reddit might severely limit the number of posts you download using this scraper even with your name appropriately in the username, so be judicious with your exploration (hence exercise 2 _first_). \n",
    "- While you might not have extensive experience with Reddit, I can be confident that there are subreddits on most imaginable topics that can be found with little challenge. However, these subs will have vastly different numbers of subscribers and activity, so bear that in mind with any interpretation when tempted to generalise what is found _beyond_ Reddit (i.e. generalising from /r/republicans to Republicans in the US). \n",
    "- You may be tempted out of curiosity to expand your data collection. You will find that this will lead to a trade off if you do not further process your data. If you have 1000 rows for headlines and 3000 for words, that's a big matrix that has to be multiplied by vectors. At some point the size of the matrix will be unnecessary as well as slow. You may need to consider different parameters for `MIN_DOC_FREQ` to get a balance between a big matrix and a meaningful one. \n",
    "- These results have not been cleared for publication with CUREC, but only for use within classroom and for illustrative purposes. Please do not upload raw reddit data to your own GitHub archive nor seek to publish these results.  (Notice that I have pre-emtively edited the .gitignore to include a `data/` folder where you can store results without uploading them). Seek advice from research.fac@oii.ox.ac.uk for use for a comparable project should you wish to publish this work. If you wish to produce a blog post or other informal analysis, this should be presented in such a way that it is not misconstrued that the University has endorsed this work for publication. \n",
    "\n",
    "# Where we are headed with this exercise \n",
    "\n",
    "### Today: \n",
    "Collect reddit data, make it robust and explore TF-IDF results. \n",
    "\n",
    "### Week 3 Day 3. Friday: \n",
    "We use contine the use of the TF-IDF matrix and introduce cosine distance. We show how to plot it using t-SNE. This might sound abstract but the results will be fascinating as we see words plotted in coherent clusters that seem to reveal inductive patterns. \n",
    "\n",
    "Worksheets will be uploaded to this repo. \n",
    "\n",
    "### Week 4 Day 1. Monday: \n",
    "We will use two simple forms of classification, k-means and Naive Bayes Clustering. You might also be familiar with LDA or 'topic modelling'. We will not cover this as the technique deserves some care to understand its internals even if it is easy to run out of the box. But it is not far as an extension from where we end up. \n",
    "\n",
    "In the lab we will then compare classification results to results from the t-SNE and exploration of distance from Friday. \n",
    "\n",
    "### Week 4 Day 2. Wednesday: \n",
    "We will introduce the `networkx` and `community` package and show how to both construct a network from threaded comments and users of these comments. This will involve two types of graphs: DAGs and Bipartite graphs. \n",
    "\n",
    "In the lab you will have code that shows how to do this with the Reddit data in general. You will have to apply this to your specific case. \n",
    "\n",
    "### Week 4 Day 3. Friday: \n",
    "In the walkthrough we will see how to create 'embeddings' as abstractions even further than t-SNE but as a next-step up from cosine distance. In fact we will see how you can use cosine distance on embeddings which allows you to do these same steps not with words, but with entire sentences or whole paragraphs. We feature this on Friday and assume that your presentations will not need to use embeddings. \n",
    "\n",
    "In the afternoon you we will have the second set of group presentations: \n",
    "- Take a current event or coherent topic that could be collected from reddit data using the requests API (or more abstract packages such as `praw`, but not entire archive dumps like PushShift, only a limited subset). \n",
    "- Look at three or more subreddits who might speak to that topic. Determine which two subs are the most similar and why? Be sure to consider not only common word use. You may define similarity in creative ways so long as they can result in calculable differences without use of ML models, external APIs, or mass labelling of data. If you can download a lexicon, you are welcome to use scoring.\n",
    "- Motivate this topic deductively. Where possible try to draw upon any existing literature on the topic and not simply abductively from current events. Consider DIKW: Find ways to produce transferable _knowledge_ rather than merely _information_ from _data_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "class RedditScraper:\n",
    "    def __init__(self, user_agent):\n",
    "        \"\"\"\n",
    "        Initialize the scraper with a user agent string.\n",
    "        Example user agent: \"SDS_textanalysis/1.0 (by /u/your_username)\"\n",
    "        \"\"\"\n",
    "        self.headers = {'User-Agent': user_agent}\n",
    "        self.base_url = \"https://api.reddit.com\"\n",
    "        \n",
    "    def get_subreddit_posts(self, subreddit, limit=100):\n",
    "        \"\"\"\n",
    "        Collect posts from a subreddit with proper pagination and rate limiting.\n",
    "        \"\"\"\n",
    "        posts = []\n",
    "        after = None\n",
    "        \n",
    "        while len(posts) < limit:\n",
    "            url = f\"{self.base_url}/r/{subreddit}/new\"\n",
    "            params = {\n",
    "                'limit': min(100, limit - len(posts)),\n",
    "                'after': after\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, headers=self.headers, params=params)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error accessing r/{subreddit}: {response.status_code}\")\n",
    "                break\n",
    "                \n",
    "            data = response.json()\n",
    "            new_posts = data['data']['children']\n",
    "            if not new_posts:\n",
    "                break\n",
    "                \n",
    "            posts.extend([post['data'] for post in new_posts])\n",
    "            after = data['data']['after']\n",
    "            \n",
    "            if not after:\n",
    "                break\n",
    "                \n",
    "            time.sleep(2)\n",
    "            \n",
    "        return posts[:limit]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize text.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def analyze_vocabulary(texts, min_freq=2):\n",
    "    \"\"\"\n",
    "    Analyze vocabulary distribution in a corpus.\n",
    "    Returns word frequencies and vocabulary statistics.\n",
    "    \"\"\"\n",
    "    # Tokenize all texts\n",
    "    words = ' '.join(texts).split()\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Calculate vocabulary statistics\n",
    "    total_words = len(words)\n",
    "    unique_words = len(word_freq)\n",
    "    \n",
    "    # Create frequency distribution DataFrame\n",
    "    freq_df = pd.DataFrame(list(word_freq.items()), columns=['word', 'frequency'])\n",
    "    freq_df['percentage'] = freq_df['frequency'] / total_words * 100\n",
    "    freq_df = freq_df.sort_values('frequency', ascending=False)\n",
    "    \n",
    "    # Calculate cumulative coverage\n",
    "    freq_df['cumulative_percentage'] = freq_df['percentage'].cumsum()\n",
    "    \n",
    "    stats = {\n",
    "        'total_words': total_words,\n",
    "        'unique_words': unique_words,\n",
    "        'words_min_freq': sum(1 for freq in word_freq.values() if freq >= min_freq),\n",
    "        'coverage_top_1000': freq_df.iloc[:1000]['frequency'].sum() / total_words * 100 if len(freq_df) >= 1000 else 100\n",
    "    }\n",
    "    \n",
    "    return freq_df, stats\n",
    "\n",
    "def analyze_subreddit(posts, max_terms=1000, min_doc_freq=2, TOP_N=10):\n",
    "    \"\"\"\n",
    "    Analyze a single subreddit's posts independently.\n",
    "    \"\"\"\n",
    "    # Combine title and selftext\n",
    "    texts = [\n",
    "        preprocess_text(post.get('title', '')) + ' ' + \n",
    "        preprocess_text(post.get('selftext', ''))\n",
    "        for post in posts\n",
    "    ]\n",
    "    \n",
    "    # Analyze vocabulary first\n",
    "    freq_df, vocab_stats = analyze_vocabulary(texts, min_freq=min_doc_freq)\n",
    "    \n",
    "    # Initialize TF-IDF vectorizer for this subreddit\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=stop_words,\n",
    "        max_features=max_terms,\n",
    "        min_df=min_doc_freq\n",
    "    )\n",
    "    \n",
    "    # Compute TF-IDF\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Get average TF-IDF scores\n",
    "    mean_tfidf = np.array(tfidf_matrix.mean(axis=0)).flatten()\n",
    "    \n",
    "    # Get top terms\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_terms = pd.DataFrame({\n",
    "        'term': feature_names,\n",
    "        'score': mean_tfidf\n",
    "    }).sort_values('score', ascending=False)\n",
    "    \n",
    "    return {\n",
    "        'vocab_stats': vocab_stats,\n",
    "        'freq_distribution': freq_df,\n",
    "        'top_terms': top_terms.head(TOP_N),\n",
    "        'vectorizer': vectorizer,\n",
    "        'matrix_shape': tfidf_matrix.shape,\n",
    "        'matrix_sparsity': 100 * (1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]))\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing r/AmItheAsshole...\n",
      "\n",
      "Vocabulary Statistics for r/AmItheAsshole:\n",
      "Total words: 198242\n",
      "Unique words: 9495\n",
      "Words appearing ≥2 times: 5218\n",
      "Coverage by top 1000 words: 86.72%\n",
      "Matrix shape: (500, 1000)\n",
      "Matrix sparsity: 90.98%\n",
      "\n",
      "Top 5 terms by TF-IDF score:\n",
      "       term     score\n",
      "444      im  0.049164\n",
      "498    like  0.042393\n",
      "728    said  0.042161\n",
      "242    dont  0.041715\n",
      "881    told  0.037435\n",
      "875    time  0.036301\n",
      "569     mom  0.036207\n",
      "338  friend  0.036137\n",
      "982   would  0.035828\n",
      "228   didnt  0.034861\n",
      "\n",
      "Analyzing r/tifu...\n",
      "\n",
      "Vocabulary Statistics for r/tifu:\n",
      "Total words: 203999\n",
      "Unique words: 12034\n",
      "Words appearing ≥2 times: 6592\n",
      "Coverage by top 1000 words: 82.91%\n",
      "Matrix shape: (500, 1000)\n",
      "Matrix sparsity: 91.06%\n",
      "\n",
      "Top 5 terms by TF-IDF score:\n",
      "      term     score\n",
      "441     im  0.047260\n",
      "495   like  0.045453\n",
      "371    got  0.035479\n",
      "879   time  0.035191\n",
      "219  didnt  0.034794\n",
      "609    one  0.034525\n",
      "352    get  0.031717\n",
      "230   dont  0.031579\n",
      "66    back  0.030013\n",
      "468   know  0.029800\n",
      "\n",
      "Analyzing r/confessions...\n",
      "\n",
      "Vocabulary Statistics for r/confessions:\n",
      "Total words: 124776\n",
      "Unique words: 8742\n",
      "Words appearing ≥2 times: 4415\n",
      "Coverage by top 1000 words: 85.28%\n",
      "Matrix shape: (500, 1000)\n",
      "Matrix sparsity: 94.26%\n",
      "\n",
      "Top 5 terms by TF-IDF score:\n",
      "      term     score\n",
      "452     im  0.072746\n",
      "513   like  0.056165\n",
      "247   dont  0.042520\n",
      "310   feel  0.038955\n",
      "487   know  0.036988\n",
      "469    ive  0.033945\n",
      "884   time  0.031785\n",
      "988  would  0.031730\n",
      "936   want  0.031557\n",
      "357    get  0.030990\n"
     ]
    }
   ],
   "source": [
    "# Example subreddits\n",
    "subreddits = ['AmItheAsshole', \n",
    "              'tifu', \n",
    "              'confessions']\n",
    "\n",
    "# Analysis parameters\n",
    "MAX_TERMS = 1000\n",
    "MIN_DOC_FREQ = 2\n",
    "LIMIT = 500\n",
    "TOP_TERMS = 10\n",
    "USERNAME = 'chicowkn' # Replace with your Reddit username\n",
    "\n",
    "# Initialize scraper\n",
    "scraper = RedditScraper(\n",
    "user_agent=f\"SDS_textanalysis/1.0 (by /u/{USERNAME})\"\n",
    ")\n",
    "\n",
    "# Analyze each subreddit independently\n",
    "results = {}\n",
    "submissions = {}\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    print(f\"\\nAnalyzing r/{subreddit}...\")\n",
    "\n",
    "    # Collect posts\n",
    "    submissions[subreddit] = scraper.get_subreddit_posts(subreddit, limit=LIMIT)\n",
    "\n",
    "    # Analyze subreddit\n",
    "    results[subreddit] = analyze_subreddit(\n",
    "        submissions[subreddit],\n",
    "        max_terms=MAX_TERMS,   # Maximum number of terms to keep\n",
    "        min_doc_freq=MIN_DOC_FREQ,   # Term must appear in at least min_doc_freq documents\n",
    "        TOP_N=TOP_TERMS   # Number of top terms to show\n",
    "    )\n",
    "\n",
    "    # Print results for this subreddit\n",
    "    print(f\"\\nVocabulary Statistics for r/{subreddit}:\")\n",
    "    print(f\"Total words: {results[subreddit]['vocab_stats']['total_words']}\")\n",
    "    print(f\"Unique words: {results[subreddit]['vocab_stats']['unique_words']}\")\n",
    "    print(f\"Words appearing ≥{MIN_DOC_FREQ} times: {results[subreddit]['vocab_stats']['words_min_freq']}\")\n",
    "    print(f\"Coverage by top {MAX_TERMS} words: {results[subreddit]['vocab_stats']['coverage_top_1000']:.2f}%\")\n",
    "    print(f\"Matrix shape: {results[subreddit]['matrix_shape']}\")\n",
    "    print(f\"Matrix sparsity: {results[subreddit]['matrix_sparsity']:.2f}%\")\n",
    "\n",
    "    print(\"\\nTop 5 terms by TF-IDF score:\")\n",
    "    print(results[subreddit]['top_terms'][['term', 'score']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def export_json(data, subreddit):\n",
    "    location_name = Path().cwd() / 'data' / f'{subreddit}.json'\n",
    "    with open(location_name, 'w') as f:\n",
    "        json.dump(data[subreddit], f, indent=2)\n",
    "\n",
    "\n",
    "def export_table(data, subreddit):\n",
    "    location_name = Path().cwd() / 'datasets' / f'{subreddit}.csv'\n",
    "    c_subreddit = pd.DataFrame(data[subreddit])\n",
    "    c_subreddit['created_utc'] = pd.to_datetime(c_subreddit['created_utc'], unit='s')\n",
    "    c_subreddit['created'] = pd.to_datetime(c_subreddit['created'], unit='s')\n",
    "    c_subreddit.to_csv(location_name, index=False)\n",
    "    \n",
    "[export_table(submissions, subreddit) for subreddit in submissions.keys()]\n",
    "[export_json(submissions, subreddit) for subreddit in submissions.keys()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import values and tables for the subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def import_subreddit(subreddit):\n",
    "    location_name = Path().cwd() / 'datasets' / f'{subreddit}.csv'\n",
    "    return pd.read_csv(location_name)\n",
    "\n",
    "all_datasets = os.listdir(Path().cwd() / 'datasets')\n",
    "all_datasets = [x.replace('.csv', '') for x in all_datasets]\n",
    "\n",
    "subreddits = pd.concat([import_subreddit(subreddit) for subreddit in all_datasets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>saved</th>\n",
       "      <th>mod_reason_title</th>\n",
       "      <th>gilded</th>\n",
       "      <th>clicked</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>permalink</th>\n",
       "      <th>stickied</th>\n",
       "      <th>url</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>media</th>\n",
       "      <th>is_video</th>\n",
       "      <th>link_flair_template_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>I (21f) make plans every week to spend time wi...</td>\n",
       "      <td>t2_5v2cf8lk</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>AITA for taking up the majority of my girlfrie...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/r/AmItheAsshole/comments/1ggg3o7/aita_for_tak...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comment...</td>\n",
       "      <td>21609113</td>\n",
       "      <td>1970-01-01 00:00:01.730385554</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>About 3 months ago I (25F) started texting a w...</td>\n",
       "      <td>t2_8kkpjzkn</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>AITA for asking my partner to speak up to her mom</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/r/AmItheAsshole/comments/1ggfxiy/aita_for_ask...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comment...</td>\n",
       "      <td>21609113</td>\n",
       "      <td>1970-01-01 00:00:01.730385114</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>I understand it may have been a slight over st...</td>\n",
       "      <td>t2_1bzv3jb425</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>AITA for telling my brother his wife’s a mania...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/r/AmItheAsshole/comments/1ggfshw/aita_for_tel...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comment...</td>\n",
       "      <td>21609113</td>\n",
       "      <td>1970-01-01 00:00:01.730384753</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>So, my daughter (16) likes to paint/draw. Her ...</td>\n",
       "      <td>t2_1bzx72t075</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>AITA for not buying my daughter’s painting whe...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/r/AmItheAsshole/comments/1ggfrx5/aita_for_not...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comment...</td>\n",
       "      <td>21609113</td>\n",
       "      <td>1970-01-01 00:00:01.730384710</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>My (ex) girlfriend (22) was in an abusive hous...</td>\n",
       "      <td>t2_r6mobw84r</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>AITA for not giving up my studies for my (ex) ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/r/AmItheAsshole/comments/1ggfk8g/aita_for_not...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comment...</td>\n",
       "      <td>21609113</td>\n",
       "      <td>1970-01-01 00:00:01.730384132</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>confessions</td>\n",
       "      <td>Hi :) \\nI'm 20F and still a virgin, have had t...</td>\n",
       "      <td>t2_ntph78me</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>20 and still a virgin</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/r/confessions/comments/1gfzvli/20_and_still_a...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/confessions/comments/...</td>\n",
       "      <td>1150825</td>\n",
       "      <td>1970-01-01 00:00:01.730329083</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>confessions</td>\n",
       "      <td>The girl only did it out of pity; I wish I had...</td>\n",
       "      <td>t2_hnvtdnpyj</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>I regret my first kiss</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/r/confessions/comments/1gfzj6o/i_regret_my_fi...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/confessions/comments/...</td>\n",
       "      <td>1150825</td>\n",
       "      <td>1970-01-01 00:00:01.730328152</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>confessions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t2_dv4gjolc</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Everyday i want to die. but for reasons beyond...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/r/confessions/comments/1gfz1nf/everyday_i_wan...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/confessions/comments/...</td>\n",
       "      <td>1150825</td>\n",
       "      <td>1970-01-01 00:00:01.730326878</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>confessions</td>\n",
       "      <td>I’m a husband in my 40s, married for over 20 y...</td>\n",
       "      <td>t2_196hbxm9sx</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Can’t Move Past My Wife’s Past, Even After 20 ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/r/confessions/comments/1gfyho9/cant_move_past...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/confessions/comments/...</td>\n",
       "      <td>1150825</td>\n",
       "      <td>1970-01-01 00:00:01.730325427</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>confessions</td>\n",
       "      <td>Ever since I(23m) was about 5 or 6 I’ve kind o...</td>\n",
       "      <td>t2_ijv6grzgr</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>I Really Love The Look Of Pregnant Woman. Is t...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/r/confessions/comments/1gfy9z6/i_really_love_...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/confessions/comments/...</td>\n",
       "      <td>1150825</td>\n",
       "      <td>1970-01-01 00:00:01.730324851</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  approved_at_utc      subreddit  \\\n",
       "0            0              NaN  AmItheAsshole   \n",
       "1            1              NaN  AmItheAsshole   \n",
       "2            2              NaN  AmItheAsshole   \n",
       "3            3              NaN  AmItheAsshole   \n",
       "4            4              NaN  AmItheAsshole   \n",
       "..         ...              ...            ...   \n",
       "45          45              NaN    confessions   \n",
       "46          46              NaN    confessions   \n",
       "47          47              NaN    confessions   \n",
       "48          48              NaN    confessions   \n",
       "49          49              NaN    confessions   \n",
       "\n",
       "                                             selftext author_fullname  saved  \\\n",
       "0   I (21f) make plans every week to spend time wi...     t2_5v2cf8lk  False   \n",
       "1   About 3 months ago I (25F) started texting a w...     t2_8kkpjzkn  False   \n",
       "2   I understand it may have been a slight over st...   t2_1bzv3jb425  False   \n",
       "3   So, my daughter (16) likes to paint/draw. Her ...   t2_1bzx72t075  False   \n",
       "4   My (ex) girlfriend (22) was in an abusive hous...    t2_r6mobw84r  False   \n",
       "..                                                ...             ...    ...   \n",
       "45  Hi :) \\nI'm 20F and still a virgin, have had t...     t2_ntph78me  False   \n",
       "46  The girl only did it out of pity; I wish I had...    t2_hnvtdnpyj  False   \n",
       "47                                                NaN     t2_dv4gjolc  False   \n",
       "48  I’m a husband in my 40s, married for over 20 y...   t2_196hbxm9sx  False   \n",
       "49  Ever since I(23m) was about 5 or 6 I’ve kind o...    t2_ijv6grzgr  False   \n",
       "\n",
       "    mod_reason_title  gilded  clicked  \\\n",
       "0                NaN       0    False   \n",
       "1                NaN       0    False   \n",
       "2                NaN       0    False   \n",
       "3                NaN       0    False   \n",
       "4                NaN       0    False   \n",
       "..               ...     ...      ...   \n",
       "45               NaN       0    False   \n",
       "46               NaN       0    False   \n",
       "47               NaN       0    False   \n",
       "48               NaN       0    False   \n",
       "49               NaN       0    False   \n",
       "\n",
       "                                                title  ...  \\\n",
       "0   AITA for taking up the majority of my girlfrie...  ...   \n",
       "1   AITA for asking my partner to speak up to her mom  ...   \n",
       "2   AITA for telling my brother his wife’s a mania...  ...   \n",
       "3   AITA for not buying my daughter’s painting whe...  ...   \n",
       "4   AITA for not giving up my studies for my (ex) ...  ...   \n",
       "..                                                ...  ...   \n",
       "45                              20 and still a virgin  ...   \n",
       "46                             I regret my first kiss  ...   \n",
       "47  Everyday i want to die. but for reasons beyond...  ...   \n",
       "48  Can’t Move Past My Wife’s Past, Even After 20 ...  ...   \n",
       "49  I Really Love The Look Of Pregnant Woman. Is t...  ...   \n",
       "\n",
       "   author_flair_text_color                                          permalink  \\\n",
       "0                      NaN  /r/AmItheAsshole/comments/1ggg3o7/aita_for_tak...   \n",
       "1                      NaN  /r/AmItheAsshole/comments/1ggfxiy/aita_for_ask...   \n",
       "2                      NaN  /r/AmItheAsshole/comments/1ggfshw/aita_for_tel...   \n",
       "3                      NaN  /r/AmItheAsshole/comments/1ggfrx5/aita_for_not...   \n",
       "4                      NaN  /r/AmItheAsshole/comments/1ggfk8g/aita_for_not...   \n",
       "..                     ...                                                ...   \n",
       "45                     NaN  /r/confessions/comments/1gfzvli/20_and_still_a...   \n",
       "46                     NaN  /r/confessions/comments/1gfzj6o/i_regret_my_fi...   \n",
       "47                     NaN  /r/confessions/comments/1gfz1nf/everyday_i_wan...   \n",
       "48                     NaN  /r/confessions/comments/1gfyho9/cant_move_past...   \n",
       "49                     NaN  /r/confessions/comments/1gfy9z6/i_really_love_...   \n",
       "\n",
       "    stickied                                                url  \\\n",
       "0      False  https://www.reddit.com/r/AmItheAsshole/comment...   \n",
       "1      False  https://www.reddit.com/r/AmItheAsshole/comment...   \n",
       "2      False  https://www.reddit.com/r/AmItheAsshole/comment...   \n",
       "3      False  https://www.reddit.com/r/AmItheAsshole/comment...   \n",
       "4      False  https://www.reddit.com/r/AmItheAsshole/comment...   \n",
       "..       ...                                                ...   \n",
       "45     False  https://www.reddit.com/r/confessions/comments/...   \n",
       "46     False  https://www.reddit.com/r/confessions/comments/...   \n",
       "47     False  https://www.reddit.com/r/confessions/comments/...   \n",
       "48     False  https://www.reddit.com/r/confessions/comments/...   \n",
       "49     False  https://www.reddit.com/r/confessions/comments/...   \n",
       "\n",
       "   subreddit_subscribers                    created_utc  num_crossposts  \\\n",
       "0               21609113  1970-01-01 00:00:01.730385554               0   \n",
       "1               21609113  1970-01-01 00:00:01.730385114               0   \n",
       "2               21609113  1970-01-01 00:00:01.730384753               1   \n",
       "3               21609113  1970-01-01 00:00:01.730384710               0   \n",
       "4               21609113  1970-01-01 00:00:01.730384132               0   \n",
       "..                   ...                            ...             ...   \n",
       "45               1150825  1970-01-01 00:00:01.730329083               0   \n",
       "46               1150825  1970-01-01 00:00:01.730328152               0   \n",
       "47               1150825  1970-01-01 00:00:01.730326878               0   \n",
       "48               1150825  1970-01-01 00:00:01.730325427               0   \n",
       "49               1150825  1970-01-01 00:00:01.730324851               0   \n",
       "\n",
       "    media is_video  link_flair_template_id  \n",
       "0     NaN    False                     NaN  \n",
       "1     NaN    False                     NaN  \n",
       "2     NaN    False                     NaN  \n",
       "3     NaN    False                     NaN  \n",
       "4     NaN    False                     NaN  \n",
       "..    ...      ...                     ...  \n",
       "45    NaN    False                     NaN  \n",
       "46    NaN    False                     NaN  \n",
       "47    NaN    False                     NaN  \n",
       "48    NaN    False                     NaN  \n",
       "49    NaN    False                     NaN  \n",
       "\n",
       "[150 rows x 104 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "domain\n",
       "https://www.reddit.com    50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Exploration: \n",
    "submissions['AmItheAsshole'][0] # Example post from the unitedkingdom subreddit\n",
    "\n",
    "url_list = [post['url'] for post in submissions['AmItheAsshole']]\n",
    "url_df = pd.DataFrame(url_list, columns=['url'])\n",
    "url_df['domain'] = url_df['url'].str.extract(r'(https?://[^/]+)')\n",
    "\n",
    "url_df['domain'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vocab_stats', 'freq_distribution', 'top_terms', 'vectorizer', 'matrix_shape', 'matrix_sparsity'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['unitedkingdom'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Declaration: \n",
    "\n",
    "Claude Sonnet 3.5 New produced much of the reddit code. I was surprised at how similar it was to my past code (knowing it was trained on GitHub I have to wonder). Several tweaks had to be made such as removing a main() function, altering the results object, altering some NLTK packages, adding the submissions dictionary and the submissions dictionary code. I kept in the `preprocess_text()` function as is, but you are encouraged to consider alternative forms of pre-processing from the walkthrough including the use of standard tokenizers, lemmatisation, and stop-words. It also used anodyne programming subreddits which I changed and I reduced the limit to 50 which is just enough to get two queries illustrating that you can get N queries through this approach. \n",
    "\n",
    "The URL code was written in VS code with co-pilot. Notably the autocomplete did an excellent job of anticipating steps with minimal prompting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
